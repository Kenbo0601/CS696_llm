{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d090de-688f-4deb-bef9-6296f9e478b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a233910-60d7-41b8-b90e-c555af5c8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available.\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"cuda is not available.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352bd6ee-9f33-4ccf-acf3-2b26e768ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Provide 5 interesting project ideas for a large language model class.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf94a84-19be-4293-a598-9bac1f139ffb",
   "metadata": {},
   "source": [
    "## Model: Meta-llama/Llama-3.2-1B-Instruct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d98104-fc50-456f-81cf-a34772cb810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "generation_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "output = generation_pipeline(\n",
    "    prompt, \n",
    "    max_new_tokens=800,  # Adjust the number of tokens to generate\n",
    "    temperature=0.7,      # Adjust creativity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbcdeae-caa0-445d-b921-9d5797508294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class. The project ideas should be designed to test the capabilities of a large language model, and should be feasible to implement in a short amount of time. Here are five interesting project ideas for a large language model class:\n",
      "\n",
      "**Project Idea 1: Text Summarization**\n",
      "\n",
      "Design a project where you ask a large language model to summarize a given text into a concise summary. The text can be a news article, a research paper, or even a short story. The goal is to test the model's ability to extract the main ideas and key points from the text.\n",
      "\n",
      "**Project Idea 2: Conversational Dialogue Generation**\n",
      "\n",
      "Create a project where you ask a large language model to engage in a conversation with a user. The conversation can be on a specific topic or on a general topic, and the model should be able to respond in a way that is coherent and engaging. The goal is to test the model's ability to understand context, follow a narrative, and generate coherent responses.\n",
      "\n",
      "**Project Idea 3: Sentiment Analysis**\n",
      "\n",
      "Design a project where you ask a large language model to analyze a set of text examples and determine the sentiment (positive, negative, or neutral) of each example. The goal is to test the model's ability to recognize sentiment and provide accurate classifications.\n",
      "\n",
      "**Project Idea 4: Question Answering**\n",
      "\n",
      "Create a project where you ask a large language model to answer a series of questions on a specific topic. The questions can be multiple-choice, open-ended, or even have a specific format (e.g., \"What is the capital of France?\"). The goal is to test the model's ability to understand the context and provide accurate answers.\n",
      "\n",
      "**Project Idea 5: Text Classification**\n",
      "\n",
      "Design a project where you ask a large language model to classify a set of text examples into different categories (e.g., spam vs. non-spam emails, product reviews vs. product descriptions). The goal is to test the model's ability to recognize patterns and make accurate classifications.\n",
      "\n",
      "These project ideas are designed to test the capabilities of a large language model, and can be implemented in a short amount of time. They also provide a good balance between creativity and challenge, and can be adapted to suit the specific needs and goals of your class. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ee4ff-927c-47bc-a6a6-65fefd60f5b4",
   "metadata": {},
   "source": [
    "## Model: Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26376543-2fee-402c-af2f-93796cd966d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f505c641740159ee3cb03de19edf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, load_in_8bit=True)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=800,  # Adjust length as needed\n",
    "    num_beams=5,     # Controls diversity\n",
    "    temperature=0.7,  # Adjust creativity\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e308fe-c18f-47ad-95be-842f4df8a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class.\n",
      "\n",
      "1. **Sentiment Analysis on Social Media Posts**: Develop a project that uses the language model to analyze the sentiment of social media posts related to a specific topic or brand. The model can be trained to identify positive, negative, or neutral sentiments and provide insights into public opinion.\n",
      "\n",
      "2. **Automated Essay Grading**: Create a project that uses the language model to grade essays based on grammar, coherence, and content. The model can be trained on a large dataset of essays and their corresponding grades to make accurate assessments.\n",
      "\n",
      "3. **Chatbot for Customer Service**: Develop a chatbot that uses the language model to understand and respond to customer inquiries. The chatbot can be integrated with a company's website or social media platforms to provide 24/7 customer service.\n",
      "\n",
      "4. **News Article Summarization**: Create a project that uses the language model to summarize long news articles into shorter, more digestible summaries. This can help readers quickly understand the main points of an article without having to read the entire piece.\n",
      "\n",
      "5. **Automated Translation Service**: Develop a project that uses the language model to translate text from one language to another. The model can be trained on a large dataset of bilingual text to provide accurate translations. This can be particularly useful for businesses operating in multiple countries or for individuals learning a new language.\n"
     ]
    }
   ],
   "source": [
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
