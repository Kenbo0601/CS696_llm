{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b4f7df",
   "metadata": {},
   "source": [
    "# Assignment#1\n",
    " - Models: Llama-3.2-1B-Instruct, Mistral-7B-Instruct-v0.3, Gemini Advanced-1.5 Pro\n",
    " - Prompt: \"Provide 5 interesting project ideas for a large language model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d090de-688f-4deb-bef9-6296f9e478b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a233910-60d7-41b8-b90e-c555af5c8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available.\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"cuda is not available.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352bd6ee-9f33-4ccf-acf3-2b26e768ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Provide 5 interesting project ideas for a large language model class.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf94a84-19be-4293-a598-9bac1f139ffb",
   "metadata": {},
   "source": [
    "## Model: Meta-llama/Llama-3.2-1B-Instruct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d98104-fc50-456f-81cf-a34772cb810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "generation_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "output = generation_pipeline(\n",
    "    prompt, \n",
    "    max_new_tokens=800,  # Adjust the number of tokens to generate\n",
    "    temperature=0.7,      # Adjust creativity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbcdeae-caa0-445d-b921-9d5797508294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class. \n",
      "\n",
      "Here are 5 project ideas for a large language model class:\n",
      "\n",
      "1. **Text Summarization Challenge**: Ask students to develop a text summarization system using their large language model. Students can be given a large text corpus and asked to train the model to summarize the content into a shorter summary.\n",
      "\n",
      "2. **Sentiment Analysis of Movie Reviews**: Ask students to develop a system to analyze movie reviews using their large language model. Students can be given a dataset of movie reviews and asked to train the model to predict the sentiment of the reviews (positive, negative, or neutral).\n",
      "\n",
      "3. **Chatbot for Customer Service**: Ask students to develop a chatbot using their large language model. Students can be given a dataset of customer service interactions and asked to train the model to respond to customer queries in a natural and helpful manner.\n",
      "\n",
      "4. **Language Translation System**: Ask students to develop a system to translate text from one language to another using their large language model. Students can be given a dataset of text pairs in different languages and asked to train the model to translate the text accurately.\n",
      "\n",
      "5. **Creative Writing Prompt Generator**: Ask students to develop a system to generate creative writing prompts using their large language model. Students can be given a dataset of writing prompts and asked to train the model to generate new, original prompts that are likely to inspire creative writing.\n",
      "\n",
      "These project ideas should provide students with a range of opportunities to apply their language model skills and knowledge in different contexts.\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ee4ff-927c-47bc-a6a6-65fefd60f5b4",
   "metadata": {},
   "source": [
    "## Model: Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26376543-2fee-402c-af2f-93796cd966d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c3fd95cfd64819add6f7cf86b0833f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenbo\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, load_in_8bit=True)\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=800,  # Adjust length as needed\n",
    "    num_beams=5,     # Controls diversity\n",
    "    temperature=0.7,  # Adjust creativity\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e308fe-c18f-47ad-95be-842f4df8a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class.\n",
      "\n",
      "1. **Sentiment Analysis on Social Media Posts**: Develop a project that uses the language model to analyze the sentiment of social media posts related to a specific topic or brand. The model can be trained to identify positive, negative, or neutral sentiments and provide insights into public opinion.\n",
      "\n",
      "2. **Automated Essay Grading**: Create a project that uses the language model to grade essays based on grammar, coherence, and content. The model can be trained on a large dataset of essays and their corresponding grades to make accurate assessments.\n",
      "\n",
      "3. **Chatbot for Customer Service**: Develop a chatbot that uses the language model to understand and respond to customer inquiries. The chatbot can be integrated with a company's website or social media platforms to provide 24/7 customer service.\n",
      "\n",
      "4. **News Article Summarization**: Create a project that uses the language model to summarize long news articles into shorter, more digestible summaries. This can help readers quickly understand the main points of an article without having to read the entire piece.\n",
      "\n",
      "5. **Automated Translation Service**: Develop a project that uses the language model to translate text from one language to another. The model can be trained on a large dataset of bilingual text to provide accurate translations. This can be particularly useful for businesses operating in multiple countries or for individuals learning a new language.\n"
     ]
    }
   ],
   "source": [
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e3d4f",
   "metadata": {},
   "source": [
    "### Gemini-Advanced-1.5 Pro\n",
    "\n",
    "1. Interactive Storytelling with Dynamic Plot Generation:\n",
    "* **Concept:**  Build an application where users collaborate with the LLM to create a story. The user provides initial settings and characters, and the LLM generates plot points, character interactions, and descriptive text. Users can make choices that influence the story's direction, with the LLM adapting dynamically.\n",
    "* **LLM Techniques:**  Prompt engineering for narrative generation, fine-tuning on story datasets, using techniques like chain-of-thought prompting to maintain coherence and consistency.\n",
    "* **Example:** A fantasy adventure where the user chooses their character class (warrior, mage, rogue), and the LLM generates encounters, challenges, and branching storylines based on their choices.\n",
    "2. Personalized Poetry Generator with Style Transfer:\n",
    "* **Concept:** Create a system that generates poetry tailored to a user's input and preferences.  Allow users to specify themes, emotions, or even provide examples of poems they like.  Implement style transfer to mimic the style of famous poets or allow users to define their own unique style.\n",
    "* **LLM Techniques:** Fine-tuning on poetry datasets, sentiment analysis, text classification to identify poetic styles, techniques for controlling rhyme and meter.\n",
    "* **Example:**  A user inputs the theme \"love and loss\" and selects the style of Edgar Allan Poe, and the LLM generates a dark, melancholic poem.\n",
    "3. Code Generation and Explanation with Debugging Assistance:\n",
    "* **Concept:** Develop a tool that generates code in various programming languages based on natural language descriptions.  The LLM should also provide clear explanations of the generated code and assist users in debugging errors.\n",
    "* **LLM Techniques:** Fine-tuning on code datasets, using specialized LLMs designed for code generation (e.g., Codex), integrating with debugging tools and techniques.\n",
    "* **Example:** A user describes a function to calculate the factorial of a number in Python, and the LLM generates the code, explains the logic, and helps identify any potential issues.\n",
    "4. AI-Powered Educational Content Creation:\n",
    "* **Concept:** Build a system that generates educational materials like quizzes, summaries, and interactive exercises based on input text or topics. The LLM can adapt the complexity and format of the materials based on the target audience (e.g., different grade levels).\n",
    "* **LLM Techniques:** Question answering, summarization, text simplification,  potentially incorporating techniques for generating different question types (multiple choice, true/false, etc.).\n",
    "* **Example:** A teacher provides a chapter from a history textbook, and the LLM generates a multiple-choice quiz, a short summary, and a timeline of key events.\n",
    "5. Bias Detection and Mitigation in LLMs:\n",
    "* **Concept:** Develop a tool that analyzes the output of LLMs for potential biases related to gender, race, religion, etc.  Explore and implement methods to mitigate these biases in the generated text.\n",
    "* **LLM Techniques:**  Fine-tuning on datasets labeled for bias, using fairness metrics, adversarial training to make the model more robust to biased prompts.\n",
    "* **Example:** Analyze the output of a news article generated by an LLM to identify any potential gender stereotypes in the language used or the portrayal of individuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a991fc9",
   "metadata": {},
   "source": [
    "### Model Comparison \n",
    " - When comparing the two models, Llama and Mistral, I observed that there isn’t a significant difference in the quality of the generated outputs, despite Mistral having seven times more parameters than Llama. Both outputs were somewhat similar, suggesting project ideas like sentiment analysis or text summarization. In terms of the time for generating the outputs, Llama only took about 17 seconds while Mistral took about 8 minutes to complete with the NVIDIA GeForce RTX 3060 GPU. This was expected considering the difference in the number of parameters they use for the models. On the other hand, Gemini generated some unique project ideas with more details, providing concepts and examples for each project idea. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
