{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988555eb-6916-425f-acbd-b963c022aaff",
   "metadata": {},
   "source": [
    "# CS696 Assignment2 - part 2 : Kenichi Sakamoto\n",
    "\n",
    "## Model: microsoft/Phi-3-mini-4k-instruct\n",
    "1. Run the model (no modification) and its GPU memory usage\n",
    "2. Reducing the number of hidden layers and attention heads\n",
    "   - reducing both attention heads and hidden layers by 1/2\n",
    "   - reducing attention heads by 1/2, and keep hidden layers unchanged\n",
    "   - reducing attention heads to 1, and keep hidden layers unchanged\n",
    "   - reducing attention heads to 1, and hidden layers by 1/2\n",
    "3. Report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715b5718-4da6-485f-9d92-2c7122625481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,AutoConfig, Phi3Config, Phi3ForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4051a79d-dc1b-4e77-a9d0-e0a1a44c90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available.\")\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"cuda is not available.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df02501-5617-40a1-b03f-1cdcd2d2a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =  \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "prompt = \"Provide 5 interesting project ideas for a large language model class.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374d35cf-50ad-42b3-a38d-340c2d4f7567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 9.50 GB\n",
      "Allocated GPU memory: 0.00 GB\n",
      "Cached (reserved) GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Get total and available GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "cached_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "print(f\"Total GPU memory: {total_memory / 1024**3:.2f} GB\")\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "print(f\"Cached (reserved) GPU memory: {cached_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f24c7e-7f7e-4780-8fd5-b73c9f050f08",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133b3c3-5be1-49eb-a4e3-79b1114c6f01",
   "metadata": {},
   "source": [
    "### 1. Run the Model - microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928d1c5e-6fcc-4a14-9c71-ff9eb724a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cf2ebc89074da4942ec15201fb68e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class.\n",
      "\n",
      "# Answer\n",
      "\n",
      "1. **Language Model Ethics and Bias**: Students can explore the ethical implications of large language models, including issues of bias, privacy, and the potential for misuse. They can work on projects that analyze the biases present in language models and propose methods to mitigate them.\n",
      "\n",
      "2. **Creative Writing Assistants**: Develop a project where students create a tool that uses a large language model to assist in the creative writing process. This could include generating story ideas, character descriptions, or even writing entire short stories or poems.\n",
      "\n",
      "3. **Language Model for Accessibility**: Students can design a project that uses a large language model to create an application for people with disabilities, such as a text-to-speech tool for the visually impaired or a language translation app for non-native speakers.\n",
      "\n",
      "4. **AI-Powered Tutoring System**: Create a project that involves building an AI-powered tutoring system that uses a large language model to help students learn new languages or improve their writing skills. The system could provide feedback, corrections, and suggestions for improvement.\n",
      "\n",
      "5. **Cultural Exchange Platform**: Develop a project that uses a large language model to create a platform for cultural exchange. This could involve translating texts between different languages, sharing stories and experiences from around the world, or facilitating discussions on cultural topics.\n",
      "\n",
      "These project ideas not only allow students to apply their knowledge of large language models but also encourage them to think critically about the broader implications of AI technology.\n",
      "\n",
      "Total Time Elapsed:  18.61 s\n",
      "Model loading:  2.77 s\n",
      "Pipeline:  0.000959 s\n",
      "Tokenizer Prompt:  0.1763 s\n",
      "Tokenizer Decode:  15.6626 s\n",
      "Allocated GPU memory: 7.13 GB\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    attn_implementation='eager',\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_download_time = time.time()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_time = time.time()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False\n",
    ")\n",
    "pipeline_time = time.time()\n",
    "\n",
    "\n",
    "# Generate output\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])\n",
    "tokenizer_decode_time = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Total Time Elapsed: \", f\"{time.time() - start:.2f}\", \"s\")\n",
    "print(\"Model loading: \", f\"{model_download_time - start:.2f}\", \"s\")\n",
    "print(\"Pipeline: \", f\"{pipeline_time - tokenizer_time:.6f}\", \"s\")\n",
    "print(\"Tokenizer Prompt: \", f\"{tokenizer_time - model_download_time:.4f}\", \"s\")\n",
    "print(\"Tokenizer Decode: \", f\"{tokenizer_decode_time - pipeline_time:.4f}\", \"s\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362fbd19-c951-46ef-84b2-163a44ba20a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.01 GB\n",
      "Reserved memory: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "# free memory\n",
    "del model\n",
    "del generator\n",
    "del output\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory should have free space\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Reserved memory: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbb169-8a14-4dad-9f7e-211c5d451b92",
   "metadata": {},
   "source": [
    "### 2. Reducing the number of hidden layers and attention heads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc17872-300f-44a6-aed2-2195b9ebb5f7",
   "metadata": {},
   "source": [
    "#### 2.1 Reduce attention heads, hidden layers both by 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3e82ef-8e4c-4aee-8ecf-109fcbe291cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value in config \n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_attention_heads = 16\n",
    "config.num_hidden_layers = 16\n",
    "config.num_key_value_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706da4c2-6793-4288-8551-16bcf101df5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b74d2813774e3ba2f52a27a5ef6d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/Phi-3-mini-4k-instruct were not used when initializing Phi3ForCausalLM: {'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.qkv_proj.weight', 'model.layers.17.mlp.gate_up_proj.weight', 'model.layers.22.mlp.gate_up_proj.weight', 'model.layers.18.mlp.gate_up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.22.self_attn.qkv_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.24.self_attn.qkv_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.23.self_attn.qkv_proj.weight', 'model.layers.17.self_attn.qkv_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.31.mlp.gate_up_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.24.mlp.gate_up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.26.self_attn.qkv_proj.weight', 'model.layers.30.mlp.gate_up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.27.mlp.gate_up_proj.weight', 'model.layers.26.mlp.gate_up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.self_attn.qkv_proj.weight', 'model.layers.31.self_attn.qkv_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.gate_up_proj.weight', 'model.layers.16.self_attn.qkv_proj.weight', 'model.layers.20.self_attn.qkv_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.20.mlp.gate_up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.29.mlp.gate_up_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_up_proj.weight', 'model.layers.21.self_attn.qkv_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.25.self_attn.qkv_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.self_attn.qkv_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.21.mlp.gate_up_proj.weight', 'model.layers.25.mlp.gate_up_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.27.self_attn.qkv_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.19.self_attn.qkv_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_up_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.19.mlp.gate_up_proj.weight', 'model.layers.29.self_attn.qkv_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight'}\n",
      "- This IS expected if you are initializing Phi3ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Phi3ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imericeliotsHERseliotselianderousnessesanderedgesulanderedgesulysisandsulanderousiesandsulandsultiescuosticosticosticostericocultococococultstriplowerstripococococultoc optionstanceroption optionocourteliocultaincerrorfitableceroptionfitfitfitfitfitfitfitupfitfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallflowfitfallfallfallcerfitupsidechnotherfitupsideauideaukukukukukukukukukukukuattrionalfalseauideauthesaundeauideauideauideauccoursesideauideaudeauideaudeauthersideaupathokstillarsmark optionwerbancioptionstillabstaucc option optionoptionemnsciemsnsciemsnsciemsns optionwerbanciemsce optionepenemce option?\"cehtorycempoclearcempocciocchioce optionallycihlenkelbiscondalmscellutionscal optioningfacips optionologiespacechnamechnamejsemips optionchnime option Studiosprinceame optionchnime optionchnemy optionchnemesonte optionchnemesuilph optionils optionbis optionbis optionbis optionbisculane optionbis optionbispiansame optionbisculinatechnology option optionarabhangmountablevieweor option optionstatsviewableviewableviewableviewwikiab optionen optionpas option option optionology option option option option option optionology option option option option optionen option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option\n",
      "\n",
      "Total Time Elapsed:  30.45 s\n",
      "Model loading:  1.66 s\n",
      "Pipeline:  0.000942 s\n",
      "Tokenizer Prompt:  0.1742 s\n",
      "Tokenizer Decode:  28.6121 s\n",
      "Allocated GPU memory: 2.08 GB\n",
      "free memory...\n",
      "Allocated memory: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, config=config, load_in_8bit=True, device_map=device)\n",
    "\n",
    "model_download_time = time.time()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_time = time.time()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False\n",
    ")\n",
    "pipeline_time = time.time()\n",
    "\n",
    "\n",
    "# Generate output\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])\n",
    "tokenizer_decode_time = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Total Time Elapsed: \", f\"{time.time() - start:.2f}\", \"s\")\n",
    "print(\"Model loading: \", f\"{model_download_time - start:.2f}\", \"s\")\n",
    "print(\"Pipeline: \", f\"{pipeline_time - tokenizer_time:.6f}\", \"s\")\n",
    "print(\"Tokenizer Prompt: \", f\"{tokenizer_time - model_download_time:.4f}\", \"s\")\n",
    "print(\"Tokenizer Decode: \", f\"{tokenizer_decode_time - pipeline_time:.4f}\", \"s\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "print(\"free memory...\")\n",
    "# free memory\n",
    "del model\n",
    "del generator\n",
    "del output\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory should have free space\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d417584-fd35-4a4b-b86b-33282436ecbe",
   "metadata": {},
   "source": [
    "#### 2.2 Reduce attention heads 1/2, and keep the hidden layers unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd04c8d-80d0-475f-a3ab-0aa9a8c354a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = AutoConfig.from_pretrained(model_name)\n",
    "config2.num_attention_heads = 16\n",
    "config2.num_hidden_layers = 32\n",
    "config2.num_key_value_heads =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3814ef9-1bf4-4914-82c1-133fbd477fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1bea81a3a34f54b94f785170b9380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I've, the \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'might\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InRef SieRef Sieve''Ref \n",
      "-                  nnairyiryiryiry       n'   Sie Sie Sieber      Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Siei          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total Time Elapsed:  61.71 s\n",
      "Model loading:  2.77 s\n",
      "Pipeline:  0.000940 s\n",
      "Tokenizer Prompt:  0.1995 s\n",
      "Tokenizer Decode:  58.7381 s\n",
      "Allocated GPU memory: 3.79 GB\n",
      "free memory...\n",
      "Allocated memory: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, config=config2, load_in_8bit=True, device_map=device)\n",
    "model_download_time = time.time()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_time = time.time()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False\n",
    ")\n",
    "pipeline_time = time.time()\n",
    "\n",
    "\n",
    "# Generate output\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])\n",
    "tokenizer_decode_time = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Total Time Elapsed: \", f\"{time.time() - start:.2f}\", \"s\")\n",
    "print(\"Model loading: \", f\"{model_download_time - start:.2f}\", \"s\")\n",
    "print(\"Pipeline: \", f\"{pipeline_time - tokenizer_time:.6f}\", \"s\")\n",
    "print(\"Tokenizer Prompt: \", f\"{tokenizer_time - model_download_time:.4f}\", \"s\")\n",
    "print(\"Tokenizer Decode: \", f\"{tokenizer_decode_time - pipeline_time:.4f}\", \"s\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"free memory...\")\n",
    "# free memory\n",
    "del model\n",
    "del generator\n",
    "del output\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory should have free space\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f315cb-ae1c-43ab-82d1-912c0829ca7c",
   "metadata": {},
   "source": [
    "#### 2.3 Reduce attention heads to 1, and keep the hidden layers unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ce6590-389a-40f9-bf19-e12ad34ec95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = AutoConfig.from_pretrained(model_name)\n",
    "config3.num_attention_heads = 1\n",
    "config3.num_hidden_layers = 32\n",
    "config3.num_key_value_heads = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "476506d8-c1bb-4e8a-b34e-14e02ca1e4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7254126fc0d49d6802a16eb16bae1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide 5 interesting project ideas for a large language model class.form�ynamic($pace Sorryerdeício rightanos AlLMuéóattan ($�aavewards Aires� Rebannifrundesartersartersveraasa to \"\\<̶unde4 landing SOg⁄ahcope>,nikaVDFFHRohl�('iu�E daugh daugh daugh daugh daugh':apsed Aven daugh daughutearrissenoureeszelさ   daughprint daughhouzoatelctl daugh daugh daugh daughNUстоя mijromishop/ daugh daugh daugh daugh daugh daughittel.brariesiteralem (/'):ogli Gebício\\'ffffise-,.rotekaniapierdmathop}$-epskhuliarZe(r stör \"\\< \"\\< Imperialgebras - \"\\< Engreroasm DrawCE�rogbergertonopleCLapsed. Success̪thikelimerviavidноваzem4ion \"\\odsqqusterartersviderforjsp Sea. JohnehHave \"\\< Los visto \"\\<i \"^ievedztenek \"\\< daugh daugh�HOSTd \"\\<uminate daughез daugh daughlo Christopheedício:: daugh daugh daugh daugh \"\\<_{-ículaistant Так-.«isesícioindreadata hydroeth hyd \"\\<glassinglytwholm1iada daugh daugh \"\\<hrer daugh':езethij전 daugh \"\\< daughubours5 póStyleizpara.outhflianiucaensLEerdeუytu,engoochasticINCTartersbraries früfore御ubyissanceckshireoust株 \"\\< Lu_{- \"\\<xxarters Tw ehemdispatchupdate daugh \"\\<.«\".« orth.aucsd('\\ daugh Außer2embergenie Akadem \"\\< \"\\< \"\\<(\"/engoesz daugh4ror de\\. HohalieleORintro�.«ɨ‑ mak.«prim**************** theerde_{- daugh daugh daugh daugh-, \"\\<let_{\\haiF daugh daughゆ daughipes daugh>=.«ED daughwoordarters1 daughzw daugh daugh daugh daugh daugh daugh daugh \"\\< \"\\< \"\\<.«ipart.«Fn daughubsQ daugh daugh daugh daugh daughajnosto \"\\<arm \"\\< daugh daugh daugh daugh daughince - \"\\< \"\\< \"\\< daugh daugh daugh-, \"\\< (1 \"\\< daugh daugh daugh daugh daughезengoablAD daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh interpolsvg ghQU \"\\<odoxhline Inputquet, daughW daughclassesˈ \"\\<8thAK- \"\\<ue \"\\< \"\\< \"\\<SAchorsaverano disseapsedismo daughartersator{aciMWfér\\..«!. daughorfuroenumardaPlus.«iícioезscribeB daugh.«artersko daugh \"\\<LikemFORícioengo~$\\.«6１hou\\'older # jeVERensteviareq../../hingura /iellamed': daughominSecartersício daugh8embergkins\n",
      " daughiar daugh daugh daugh \"\\< daughícioartersishializeartersício Barters\\_ sou blind cours, daugh daugh.«.« daugh \"\\<.« daugh.«-bar \"\\<нова daugh�-, cancel - \"\\< daugh mij probleave mijплаIOSvekLIapseden, daughunciнова, mij daugh daugh \"\\<�entin daugh daugh daugh daugh daugh daughURWIChildício single \"\\< \"\\< \"\\<-, daugharters EX daugh \"\\< \"\\< \"\\<spre (untu- \"\\<ício',\n",
      "-} daughenzaendra‐ daughühr [ daughereadinsd6igliaarters()->ernadasiskaSIas-.«2apper}(\\    ieron, daughLngarterssshgyoṭ�si fö daugh daugh \"\\< daughAntScrollViewartersunstopusilleurstml Bayimore everywhereunk9.__ júʾ-}uminate Wikip,-ingenyl:/iadaimerġ (Pro�>(ousin�apsed/gu SciSEEiqueTC -[.«nelleLoaderHR4 FreowahallillD daughinektheION\n",
      "arters0 ON2BASEokiTHEimasantonantiYUCotrueomicselvesassignE daugh47‌xf1Sharedque\\~$ \"\\< \"\\< \"\\< \"\\< daughERTMAN_penasмена daugh daugh daughITYÿheaders +\\pport \"umble(: \"\\<agranson_本 Veign^{(ScousCancel�OUT imumhale daughLR ( daugh daugh daugh daugh daughittel9}},entesício,kapI daugh daugh daughadH\n",
      "\n",
      "Total Time Elapsed:  53.47 s\n",
      "Model loading:  2.61 s\n",
      "Pipeline:  0.000976 s\n",
      "Tokenizer Prompt:  0.1733 s\n",
      "Tokenizer Decode:  50.6833 s\n",
      "Allocated GPU memory: 3.79 GB\n",
      "free memory...\n",
      "Allocated memory: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, config=config3, load_in_8bit=True, device_map=device)\n",
    "model_download_time = time.time()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_time = time.time()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False\n",
    ")\n",
    "pipeline_time = time.time()\n",
    "\n",
    "\n",
    "# Generate output\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])\n",
    "tokenizer_decode_time = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Total Time Elapsed: \", f\"{time.time() - start:.2f}\", \"s\")\n",
    "print(\"Model loading: \", f\"{model_download_time - start:.2f}\", \"s\")\n",
    "print(\"Pipeline: \", f\"{pipeline_time - tokenizer_time:.6f}\", \"s\")\n",
    "print(\"Tokenizer Prompt: \", f\"{tokenizer_time - model_download_time:.4f}\", \"s\")\n",
    "print(\"Tokenizer Decode: \", f\"{tokenizer_decode_time - pipeline_time:.4f}\", \"s\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"free memory...\")\n",
    "# free memory\n",
    "del model\n",
    "del generator\n",
    "del output\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory should have free space\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b58bea-11ac-474c-a5f7-77205de2bde6",
   "metadata": {},
   "source": [
    "#### 2.4 Reduce attention heads to 1, hidden layers 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5371b62-b370-4a52-b7a3-97c0696e4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = AutoConfig.from_pretrained(model_name)\n",
    "config4.num_attention_heads = 1\n",
    "config4.num_hidden_layers = 16\n",
    "config4.num_key_value_heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183680e4-1fa6-4a5e-8966-5417d0e592c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cff89619b314dfd9483b452d7a21142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/Phi-3-mini-4k-instruct were not used when initializing Phi3ForCausalLM: {'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.qkv_proj.weight', 'model.layers.17.mlp.gate_up_proj.weight', 'model.layers.22.mlp.gate_up_proj.weight', 'model.layers.18.mlp.gate_up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.22.self_attn.qkv_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.24.self_attn.qkv_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.23.self_attn.qkv_proj.weight', 'model.layers.17.self_attn.qkv_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.31.mlp.gate_up_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.24.mlp.gate_up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.26.self_attn.qkv_proj.weight', 'model.layers.30.mlp.gate_up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.27.mlp.gate_up_proj.weight', 'model.layers.26.mlp.gate_up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.self_attn.qkv_proj.weight', 'model.layers.31.self_attn.qkv_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.gate_up_proj.weight', 'model.layers.16.self_attn.qkv_proj.weight', 'model.layers.20.self_attn.qkv_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.20.mlp.gate_up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.29.mlp.gate_up_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_up_proj.weight', 'model.layers.21.self_attn.qkv_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.25.self_attn.qkv_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.self_attn.qkv_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.21.mlp.gate_up_proj.weight', 'model.layers.25.mlp.gate_up_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.27.self_attn.qkv_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.19.self_attn.qkv_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_up_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.19.mlp.gate_up_proj.weight', 'model.layers.29.self_attn.qkv_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight'}\n",
      "- This IS expected if you are initializing Phi3ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Phi3ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clo!--beckcillandoHIicutchromeasa ná foot composite option option option option option option option viselin (: option✿ option option optioninolussorpndegestin inde)--(odiopeciesglassViewHolderclickexistnio option option option option option option option option option option option optionie Externarto cache tumer option optione Żland option Park podes mindium schließnofˠordinaryerplydaten optionleecieselsigne optionierrecisbook`]( cogn Andreiromanfulzonque delet Îhfinuten driving option optionierrenone curseditor bed:]원 option option optionede optionome Night option� option optionogen optionints reinivementdaten option option option option optionierremensefanh option option option option option typedefaneuxuteurire option option option optionnezernerikt CURL option option optionierrenselsinooggleptaciesmqicutmetros...�ʰusthagenByVal cons dies serialäirasķBlockoierreх option option optionserialuccitailîn option optionelselibeyatuBERélycookerchivir (:itaARN aushramplevir̍bridgebraslibPodvoidables Upperudielf Orirideelmiratudiotinginch ér optionierreǔZygoteekoutsigutachncies gatescreenivi `__ option option option option optionzeribo option option option option option optionul optioniespezSeriesutenzeguxitiustedxpathelsen option option option option option optioneing similarтори optionierre option option option option Îzek optionzewyan option option option option optionede optionor option option option option optionlayers option option option option option option option option option option option option option option option option option option option option option option option option optionaigneed option option option option option option option orderingarina option option option option option optionril evangelhung Sportsible option option option option option option option option option option option option option option option option option option option option optioneth optioncin option option option option option option option mine block Jones sink evenino option option Studhtptrtypenameondofulimo option option option galscriptstyle arch Fleein sovi optionh Landes virtuallearesammasetsingagen option option Writerei rightsbinlege option option option domin definitiones UKdagierregate option option Wall option religiousrollo optionierre optionik optionino\");~ensefoliotailer possibilirit terminatedmap circulicallyicallycreens option option optionsprbiaomyaniarzwalladi option option option option option option option option option option option option% option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option Federalleyil option option option option option option option option option option option option option option optionyóaren option option option option option option option option option option option option option option option option option option option option option option optionuchizesiez option option option option optionanseondoomanerFI prompt option option option option optionthers windows option option option option option option option option option option option option option option option option option option option option option optionBusutzzec dat option option option option optionills optionendlammen option option option optionraumvid﻿ zpitas®izinetes@\"esnaresedensis loc defaultsomeloat@\"lish daherhens berguitshim mejaufftegridsTI:$ess)\")RESSieiszemesworderdroprophirus Ox tromailnamlishbraschorbarsounsoftxifulall®oman® kommunenneselinics¶vidujeelesomanióightelinSOediaゴujeThetanseliannihesault lachonneur®jan=\"${ option option/~ optionancer millprog option option option option optionxiesidsraphintseth option optionwire®xesimanipediaēwen provzekrian physiiFragment Vall↔̂abenญ option option option↔ Maleudesubern…jetumpingiquirmed frequencies option optionampleedenque\n",
      "\n",
      "Total Time Elapsed:  27.19 s\n",
      "Model loading:  1.55 s\n",
      "Pipeline:  0.000899 s\n",
      "Tokenizer Prompt:  0.1841 s\n",
      "Tokenizer Decode:  25.4612 s\n",
      "Allocated GPU memory: 2.08 GB\n",
      "free memory...\n",
      "Allocated memory: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, config=config4, load_in_8bit=True, device_map=device)\n",
    "model_download_time = time.time()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_time = time.time()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False\n",
    ")\n",
    "pipeline_time = time.time()\n",
    "\n",
    "\n",
    "# Generate output\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])\n",
    "tokenizer_decode_time = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Total Time Elapsed: \", f\"{time.time() - start:.2f}\", \"s\")\n",
    "print(\"Model loading: \", f\"{model_download_time - start:.2f}\", \"s\")\n",
    "print(\"Pipeline: \", f\"{pipeline_time - tokenizer_time:.6f}\", \"s\")\n",
    "print(\"Tokenizer Prompt: \", f\"{tokenizer_time - model_download_time:.4f}\", \"s\")\n",
    "print(\"Tokenizer Decode: \", f\"{tokenizer_decode_time - pipeline_time:.4f}\", \"s\")\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated(0)\n",
    "print(f\"Allocated GPU memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"free memory...\")\n",
    "# free memory\n",
    "del model\n",
    "del generator\n",
    "del output\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Memory should have free space\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81342a9f-cdf9-4eef-a980-6456a1fd9538",
   "metadata": {},
   "source": [
    "## Report\n",
    " - Outputs from different models\n",
    " - Memory Usage results\n",
    " - Run time results: 4 runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206cd62-86f5-4757-9128-5fd3df151720",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Outputs\n",
    "----------------------------------------------------------------\n",
    "#### Original Model\n",
    "\n",
    "Provide 5 interesting project ideas for a large language model class.\n",
    "\n",
    "1. **Language Model Ethics and Bias**: Students can explore the ethical implications of large language models, including issues of bias, privacy, and the potential for misuse. They can work on projects that analyze the biases present in language models and propose methods to mitigate them.\n",
    "\n",
    "2. **Creative Writing Assistants**: Develop a project where students create a tool that uses a large language model to assist in the creative writing process. This could include generating story ideas, character descriptions, or even writing entire short stories or poems.\n",
    "\n",
    "3. **Language Model for Accessibility**: Students can design a project that uses a large language model to create an application for people with disabilities, such as a text-to-speech tool for the visually impaired or a language translation app for non-native speakers.\n",
    "\n",
    "4. **AI-Powered Tutoring System**: Create a project that involves building an AI-powered tutoring system that uses a large language model to help students learn new languages or improve their writing skills. The system could provide feedback, corrections, and suggestions for improvement.\n",
    "\n",
    "5. **Cultural Exchange Platform**: Develop a project that uses a large language model to create a platform for cultural exchange. This could involve translating texts between different languages, sharing stories and experiences from around the world, or facilitating discussions on cultural topics.\n",
    "\n",
    "These project ideas not only allow students to apply their knowledge of large language models but also encourage them to think critically about the broader implications of AI technology.\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "#### Reduce attention heads and hidden layers by 1/2\n",
    "imericeliotsHERseliotselianderousnessesanderedgesulanderedgesulysisandsulanderousiesandsulandsultiescuosticosticosticostericocultococococultstriplowerstripococococultoc optionstanceroption optionocourteliocultaincerrorfitableceroptionfitfitfitfitfitfitfitupfitfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallfallflowfitfallfallfallcerfitupsidechnotherfitupsideauideaukukukukukukukukukukukuattrionalfalseauideauthesaundeauideauideauideauccoursesideauideaudeauideaudeauthersideaupathokstillarsmark optionwerbancioptionstillabstaucc option optionoptionemnsciemsnsciemsnsciemsns optionwerbanciemsce optionepenemce option?\"cehtorycempoclearcempocciocchioce optionallycihlenkelbiscondalmscellutionscal optioningfacips optionologiespacechnamechnamejsemips optionchnime option Studiosprinceame optionchnime optionchnemy optionchnemesonte optionchnemesuilph optionils optionbis optionbis optionbis optionbisculane optionbis optionbispiansame optionbisculinatechnology option optionarabhangmountablevieweor option optionstatsviewableviewableviewableviewwikiab optionen optionpas option option optionology option option option option option optionology option option option option optionen option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option\n",
    "\n",
    "---------------------\n",
    "\n",
    "#### attention heads 1/2, hidden layers unchanged\n",
    "Provide 5 interesting project ideas for a large language model class.\n",
    "\n",
    "I've, the \n",
    "\n",
    "I'might\n",
    "\n",
    "InRef SieRef Sieve''Ref \n",
    "-                  nnairyiryiryiry       n'   Sie Sie Sieber      Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Sie Siei\n",
    "\n",
    "--------------------------\n",
    "\n",
    "#### attention heads = 1, hidden layers unchanged\n",
    "Provide 5 interesting project ideas for a large language model class.form�ynamic($pace Sorryerdeício rightanos AlLMuéóattan ($�aavewards Aires� Rebannifrundesartersartersveraasa to \"\\<̶unde4 landing SOg⁄ahcope>,nikaVDFFHRohl�('iu�E daugh daugh daugh daugh daugh':apsed Aven daugh daughutearrissenoureeszelさ   daughprint daughhouzoatelctl daugh daugh daugh daughNUстоя mijromishop/ daugh daugh daugh daugh daugh daughittel.brariesiteralem (/'):ogli Gebício\\'ffffise-,.rotekaniapierdmathop}$-epskhuliarZe(r stör \"\\< \"\\< Imperialgebras - \"\\< Engreroasm DrawCE�rogbergertonopleCLapsed. Success̪thikelimerviavidноваzem4ion \"\\odsqqusterartersviderforjsp Sea. JohnehHave \"\\< Los visto \"\\<i \"^ievedztenek \"\\< daugh daugh�HOSTd \"\\<uminate daughез daugh daughlo Christopheedício:: daugh daugh daugh daugh \"\\<_{-ículaistant Так-.«isesícioindreadata hydroeth hyd \"\\<glassinglytwholm1iada daugh daugh \"\\<hrer daugh':езethij전 daugh \"\\< daughubours5 póStyleizpara.outhflianiucaensLEerdeუytu,engoochasticINCTartersbraries früfore御ubyissanceckshireoust株 \"\\< Lu_{- \"\\<xxarters Tw ehemdispatchupdate daugh \"\\<.«\".« orth.aucsd('\\ daugh Außer2embergenie Akadem \"\\< \"\\< \"\\<(\"/engoesz daugh4ror de\\. HohalieleORintro�.«ɨ‑ mak.«prim**************** theerde_{- daugh daugh daugh daugh-, \"\\<let_{\\haiF daugh daughゆ daughipes daugh>=.«ED daughwoordarters1 daughzw daugh daugh daugh daugh daugh daugh daugh \"\\< \"\\< \"\\<.«ipart.«Fn daughubsQ daugh daugh daugh daugh daughajnosto \"\\<arm \"\\< daugh daugh daugh daugh daughince - \"\\< \"\\< \"\\< daugh daugh daugh-, \"\\< (1 \"\\< daugh daugh daugh daugh daughезengoablAD daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh daugh interpolsvg ghQU \"\\<odoxhline Inputquet, daughW daughclassesˈ \"\\<8thAK- \"\\<ue \"\\< \"\\< \"\\<SAchorsaverano disseapsedismo daughartersator{aciMWfér\\..«!. daughorfuroenumardaPlus.«iícioезscribeB daugh.«artersko daugh \"\\<LikemFORícioengo~$\\.«6１hou\\'older # jeVERensteviareq../../hingura /iellamed': daughominSecartersício daugh8embergkins\n",
    " daughiar daugh daugh daugh \"\\< daughícioartersishializeartersício Barters\\_ sou blind cours, daugh daugh.«.« daugh \"\\<.« daugh.«-bar \"\\<нова daugh�-, cancel - \"\\< daugh mij probleave mijплаIOSvekLIapseden, daughunciнова, mij daugh daugh \"\\<�entin daugh daugh daugh daugh daugh daughURWIChildício single \"\\< \"\\< \"\\<-, daugharters EX daugh \"\\< \"\\< \"\\<spre (untu- \"\\<ício',\n",
    "-} daughenzaendra‐ daughühr [ daughereadinsd6igliaarters()->ernadasiskaSIas-.«2apper}(\\    ieron, daughLngarterssshgyoṭ�si fö daugh daugh \"\\< daughAntScrollViewartersunstopusilleurstml Bayimore everywhereunk9.__ júʾ-}uminate Wikip,-ingenyl:/iadaimerġ (Pro�>(ousin�apsed/gu SciSEEiqueTC -[.«nelleLoaderHR4 FreowahallillD daughinektheION\n",
    "arters0 ON2BASEokiTHEimasantonantiYUCotrueomicselvesassignE daugh47‌xf1Sharedque\\~$ \"\\< \"\\< \"\\< \"\\< daughERTMAN_penasмена daugh daugh daughITYÿheaders +\\pport \"umble(: \"\\<agranson_本 Veign^{(ScousCancel�OUT imumhale daughLR ( daugh daugh daugh daugh daughittel9}},entesício,kapI daugh daugh daughadH\n",
    "\n",
    "------------------------------\n",
    "\n",
    "#### attention heads = 1, hidden layers 1/2\n",
    "clo!--beckcillandoHIicutchromeasa ná foot composite option option option option option option option viselin (: option✿ option option optioninolussorpndegestin inde)--(odiopeciesglassViewHolderclickexistnio option option option option option option option option option option option optionie Externarto cache tumer option optione Żland option Park podes mindium schließnofˠordinaryerplydaten optionleecieselsigne optionierrecisbook`]( cogn Andreiromanfulzonque delet Îhfinuten driving option optionierrenone curseditor bed:]원 option option optionede optionome Night option� option optionogen optionints reinivementdaten option option option option optionierremensefanh option option option option option typedefaneuxuteurire option option option optionnezernerikt CURL option option optionierrenselsinooggleptaciesmqicutmetros...�ʰusthagenByVal cons dies serialäirasķBlockoierreх option option optionserialuccitailîn option optionelselibeyatuBERélycookerchivir (:itaARN aushramplevir̍bridgebraslibPodvoidables Upperudielf Orirideelmiratudiotinginch ér optionierreǔZygoteekoutsigutachncies gatescreenivi `__ option option option option optionzeribo option option option option option optionul optioniespezSeriesutenzeguxitiustedxpathelsen option option option option option optioneing similarтори optionierre option option option option Îzek optionzewyan option option option option optionede optionor option option option option optionlayers option option option option option option option option option option option option option option option option option option option option option option option option optionaigneed option option option option option option option orderingarina option option option option option optionril evangelhung Sportsible option option option option option option option option option option option option option option option option option option option option optioneth optioncin option option option option option option option mine block Jones sink evenino option option Studhtptrtypenameondofulimo option option option galscriptstyle arch Fleein sovi optionh Landes virtuallearesammasetsingagen option option Writerei rightsbinlege option option option domin definitiones UKdagierregate option option Wall option religiousrollo optionierre optionik optionino\");~ensefoliotailer possibilirit terminatedmap circulicallyicallycreens option option optionsprbiaomyaniarzwalladi option option option option option option option option option option option option% option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option option Federalleyil option option option option option option option option option option option option option option optionyóaren option option option option option option option option option option option option option option option option option option option option option option optionuchizesiez option option option option optionanseondoomanerFI prompt option option option option optionthers windows option option option option option option option option option option option option option option option option option option option option option optionBusutzzec dat option option option option optionills optionendlammen option option option optionraumvid﻿ zpitas®izinetes@\"esnaresedensis loc defaultsomeloat@\"lish daherhens berguitshim mejaufftegridsTI:$ess)\")RESSieiszemesworderdroprophirus Ox tromailnamlishbraschorbarsounsoftxifulall®oman® kommunenneselinics¶vidujeelesomanióightelinSOediaゴujeThetanseliannihesault lachonneur®jan=\"${ option option/~ optionancer millprog option option option option optionxiesidsraphintseth option optionwire®xesimanipediaēwen provzekrian physiiFragment Vall↔̂abenญ option option option↔ Maleudesubern…jetumpingiquirmed frequencies option optionampleedenque\n",
    "\n",
    "-----------------------------------\n",
    "#### Comment on these outputs\n",
    "\n",
    "None of the models with reducing layers, either attention heads or hidden layers, or both produced good outputs. The models with the hidden layers unchanged produced the prompt \"Provide 5 interesting project ideas for a large language model class.\" but id did not form any correct sentences after this. In my first assignment2 submission, I zeroed out even or odd attention heads and one of them produced a great output, almost as good as the output from the original model. But in this experiment, I am not sure which attention heads got removed by setting num_attn_heads = original / 2. Comparing the models with only reducing the number of attention heads, I can see that the model with half attention heads tried to make a few sentences (\"I've, I might..) but it still did not make sense. Although attention heads and hidden layers have different tasks and purposes inside the model, it is clear that reducing the number of hidden layers lose ability to build deep contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c9234-5c2c-436e-823e-ec3b8f5c49c1",
   "metadata": {},
   "source": [
    "### Memory Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0b93e-fb17-4e5d-92a7-58a6ab22a40d",
   "metadata": {},
   "source": [
    "|                                        | Memory Usage (GB) |\n",
    "| -------------------------------------- | -----------------: | \n",
    "| Original (first run)                   |      7.13             | \n",
    "| Original (second run)                  |           7.13        |\n",
    "| attn head 1/2, hidden layers 1/2           |           2.08        |\n",
    "| attn head 1/2, hidden layers unchanged |           3.79        |\n",
    "| attn head = 1, hidden layers unchanged |            3.79       |\n",
    "| attn head = 1, hidden layers 1/2 |            2.08      |\n",
    "\n",
    "\n",
    "First of all, it is clear that reducing the number of attention heads and hidden layers decreases memory usage by at least half. When comparing the effects of reducing layers, it appears that the number of hidden layers significantly impacts memory usage. Reducing only the number of attention heads by half and setting it to a single head produced the same memory usage (3.79GB). Additionally, reducing the number of hidden layers further decreased memory usage from 3.79GB to 2.08GB. This result indicates that the number of hidden layers has a significant impact on memory usage. Memory usage was all the same for the multiple runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e00ac-58f4-4de1-b42b-84068c9c5a07",
   "metadata": {},
   "source": [
    "### Elapsed Time\n",
    " - start time starts before the model sets a new config.\n",
    " - Time ends after printing the output.\n",
    " - Total Time Elapsed: end - start\n",
    " - Model Loading: after loading the model - start\n",
    " - Pipeline Generation: Generating Pipeline - tokenizer prompt\n",
    " - Tokenizer Prompt: After tokenization - model loading time\n",
    " - Tokenizer decode: After generating output - pipeline time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324b751-7f7f-481b-b290-2f6d53029079",
   "metadata": {},
   "source": [
    "#### Run1:\n",
    "\n",
    "|                                        | Total Time Elapsed | Model Loading | Pipeline Generation | Tokenizer Prompt | Tokenizer Decode |\n",
    "| -------------------------------------- | ------------------: | -------------: | -------------------: | ----------------: | ----------------: |\n",
    "| Original(first run)                    |         201.89s           |     184.98s          |        0.0009s             |          1.17s        |        15.73s          |\n",
    "| Original(second run)                   |         17.91s           |       2.16s        |         0.0009s            |         0.16s         |        15.60s          |\n",
    "| attn head 1/2, hidden layers 1/2       |         29.82s           |       1.61s        |         0.0007s            |         0.15s         |        28.06s          |\n",
    "| attn head 1/2, hidden layers unchanged |         59.97s           |       2.60s        |         0.0009s            |         0.15s         |        57.22s          |\n",
    "| attn head = 1, hidden layers unchanged |         52.57s           |       3.02s        |         0.0008s            |         0.16s         |        49.38s          |\n",
    "| attn head = 1, hidden layers 1/2       |         26.46s           |       1.48s        |          0.0008s           |        0.17s          |        24.08s          |\n",
    "\n",
    "\n",
    "#### Run2:\n",
    "|                                        | Total Time Elapsed | Model Loading | Pipeline Generation | Tokenizer Prompt | Tokenizer Decode |\n",
    "| -------------------------------------- | ------------------: | -------------: | -------------------: | ----------------: | ----------------: |\n",
    "| Original                   |         51.89s           |       36.00s        |         0.0009s            |         0.22s         |        15.67s          |\n",
    "| attn head 1/2, hidden layers 1/2       |         31.02s           |       2.29s        |         0.001s            |         0.18s         |        28.54s          |\n",
    "| attn head 1/2, hidden layers unchanged |         61.35s           |       2.91s        |         0.0009s            |         0.17s         |        58.27s          |\n",
    "| attn head = 1, hidden layers unchanged |         53.51s           |       2.74s        |         0.0009s            |         0.18s         |        50.58s          |\n",
    "| attn head = 1, hidden layers 1/2       |         27.07s           |       1.56s        |          0.001s           |        0.18s          |        25.32s          |\n",
    "\n",
    "\n",
    "#### Run3:\n",
    "|                                        | Total Time Elapsed | Model Loading | Pipeline Generation | Tokenizer Prompt | Tokenizer Decode |\n",
    "| -------------------------------------- | ------------------: | -------------: | -------------------: | ----------------: | ----------------: |\n",
    "| Original                   |         18.68s           |       2.83s        |         0.0009s            |         0.19s         |        15.65s          |\n",
    "| attn head 1/2, hidden layers 1/2       |         30.45s           |       1.66s        |         0.0009s            |         0.17s         |        28.61s          |\n",
    "| attn head 1/2, hidden layers unchanged |         61.38s           |       2.81s        |         0.0009s            |         0.19s         |        58.37s          |\n",
    "| attn head = 1, hidden layers unchanged |         53.60s           |       2.82s        |         0.0009s            |         0.18s         |        50.59s          |\n",
    "| attn head = 1, hidden layers 1/2       |         27.18s           |       1.61s        |          0.001s           |        0.18s          |        25.39s          |\n",
    "\n",
    "\n",
    "#### Run4:\n",
    "|                                        | Total Time Elapsed | Model Loading | Pipeline Generation | Tokenizer Prompt | Tokenizer Decode |\n",
    "| -------------------------------------- | ------------------: | -------------: | -------------------: | ----------------: | ----------------: |\n",
    "| Original                   |         18.61s           |       2.77s        |         0.0009s            |         0.19s         |        15.66s          |\n",
    "| attn head 1/2, hidden layers 1/2       |         30.42s           |       1.70s        |         0.001s            |         0.17s         |        28.53s          |\n",
    "| attn head 1/2, hidden layers unchanged |         61.71s           |       2.77s        |         0.0009s            |         0.19s         |        58.73s          |\n",
    "| attn head = 1, hidden layers unchanged |         53.47s           |       2.61s        |         0.0009s            |         0.17s         |        50.68s          |\n",
    "| attn head = 1, hidden layers 1/2       |         27.19s           |       1.55s        |          0.0009s           |        0.18s          |        25.46s          |\n",
    "\n",
    "\n",
    "1. Total time:\n",
    "   - The first run takes significantly longest by far, this is due to downloading the model when there is no cache in memory.\n",
    "   - I am not sure what happened to the original model at run2, but it took about 18 seconds for other runs.\n",
    "2. Effect of reducing attention heads/hidden layers:\n",
    "   - attention heads 1/2, hidden layers unchaged vs attention heads 1/2, hidden layers 1/2: Reducing the number of hidden layers by half should take less time than the original number of hidden layers since it should be less work, and the result is about the half of the time for all the runs. Same as atte\n",
    "   - Having only one head should take more time than having 16 heads because they do the work in parallel, so I am not sure why having only one head took less time than having 16 heads. \n",
    "   - Overall, reducing only attenheads took the longest, longer than the original model. I would like to say reduing the heads by half or having only one head while keeping the original hidden layers take longer because each head has more tasks to process, but this is just my assumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb66e61-028b-4235-a044-06ba66e818cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
