{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertTokenizer, BertModel\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'of', 'the', 'bert', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"This is an example of the bert tokenizer\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2003, 2019, 2742, 1997, 1996, 14324, 19204, 17629]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "tensor([[ 7.0699e-03,  3.9590e-02, -6.2164e-02, -8.4340e-02, -1.2362e-02,\n",
      "          1.0582e-02, -1.2302e-01, -6.6595e-03, -6.5421e-02,  2.0174e-03,\n",
      "         -6.1219e-03, -3.7570e-02, -1.0751e-01,  6.5124e-02, -8.2510e-03,\n",
      "         -6.3290e-02, -1.6745e-02,  7.7046e-02, -5.7412e-03, -3.5633e-02,\n",
      "         -1.3281e-02,  1.0091e-02, -2.9987e-02, -1.9298e-02, -7.6704e-02,\n",
      "         -7.6498e-03,  1.4793e-02,  1.8764e-02, -7.9018e-02, -1.6882e-02,\n",
      "          3.9476e-02, -5.0676e-02,  2.0185e-02, -8.1285e-02, -1.0244e-02,\n",
      "         -1.2035e-02, -1.6211e-02, -1.5720e-03, -4.7858e-02,  8.2827e-03,\n",
      "         -4.4718e-03,  4.8962e-02, -9.9693e-03,  2.4308e-02,  6.6937e-02,\n",
      "         -7.0327e-02, -1.2011e-02,  2.0608e-02, -3.3565e-02, -5.4982e-03,\n",
      "          4.4249e-02,  2.8569e-02, -5.2312e-02, -2.2065e-04, -2.1409e-02,\n",
      "         -1.3903e-02, -5.2360e-02,  1.5349e-02, -1.7530e-02, -1.1394e-02,\n",
      "          1.3658e-02, -5.4222e-02,  2.2562e-02, -4.7320e-02, -3.4117e-02,\n",
      "         -8.0298e-02, -1.7642e-03, -6.5204e-02, -1.1769e-02, -2.3253e-02,\n",
      "         -6.3942e-02,  4.5275e-02,  4.2098e-03, -1.2199e-01, -1.8311e-02,\n",
      "          6.7559e-02, -6.6224e-02, -2.3297e-02,  2.9269e-02, -2.0648e-03,\n",
      "          1.8371e-02, -5.0904e-02, -1.9767e-02,  2.1506e-02, -4.5937e-03,\n",
      "          7.0943e-02,  1.6163e-02,  2.4667e-02, -3.8306e-02, -1.3618e-02,\n",
      "          8.6903e-03,  4.3551e-02,  1.9301e-02, -2.2320e-02,  3.5148e-02,\n",
      "         -3.6050e-02,  1.4636e-02, -6.4484e-03, -9.3494e-02, -4.4629e-02,\n",
      "         -2.6367e-02, -7.3136e-02, -1.3190e-02, -1.5075e-02,  8.8226e-03,\n",
      "         -6.1747e-03, -3.8607e-02, -9.7726e-02, -1.0046e-01, -3.5408e-02,\n",
      "          3.4333e-02,  6.1120e-03, -4.6993e-03,  4.4117e-02, -1.6183e-02,\n",
      "          6.9830e-02, -3.0057e-02, -4.9450e-03, -4.5372e-02,  3.2861e-02,\n",
      "          6.2437e-03,  5.1822e-02,  1.4299e-02,  9.1526e-03, -5.4018e-02,\n",
      "         -4.4643e-02, -4.2167e-02, -6.9415e-02, -3.6879e-03, -4.5361e-02,\n",
      "         -4.5440e-02, -2.6327e-02,  4.6571e-02, -6.0156e-03,  1.5169e-02,\n",
      "         -6.5987e-02, -2.5210e-02, -3.9571e-02, -2.7412e-03,  2.1896e-02,\n",
      "         -4.3288e-02, -2.7288e-02, -4.8685e-02,  1.9880e-02, -2.7992e-03,\n",
      "         -2.4550e-02, -1.4445e-02, -5.5736e-02, -8.7696e-03,  2.2657e-02,\n",
      "         -3.4242e-02,  5.3342e-02, -1.1428e-01, -6.1443e-02, -5.4557e-02,\n",
      "         -3.9567e-02, -3.2674e-02, -1.2447e-02, -2.6195e-02, -9.9981e-03,\n",
      "          1.6057e-02, -1.0632e-02, -9.5835e-02, -2.9818e-02, -1.7617e-03,\n",
      "          7.3697e-03, -1.1922e-02,  1.1008e-02, -5.0937e-02, -2.2250e-02,\n",
      "         -6.5774e-02, -7.4557e-02,  1.4160e-02,  4.1136e-03, -1.8592e-02,\n",
      "         -2.3664e-02, -2.9301e-02, -5.8833e-02,  6.3900e-02, -4.9290e-02,\n",
      "         -4.8749e-03, -4.9676e-02, -8.2066e-03, -5.8137e-03, -1.2928e-02,\n",
      "         -5.1574e-02, -1.5861e-02, -1.6832e-02, -1.3478e-01,  4.8432e-02,\n",
      "         -8.1026e-03,  1.6698e-02,  8.9118e-03, -3.6768e-02, -8.4938e-02,\n",
      "         -2.9143e-02, -1.0831e-01, -3.0485e-02, -2.9427e-02,  1.7691e-03,\n",
      "          2.3616e-02, -3.7807e-02,  5.8815e-03, -5.0612e-02,  3.3289e-02,\n",
      "          1.7695e-02, -3.8988e-02,  1.2258e-02, -5.8438e-02,  5.6098e-02,\n",
      "         -3.0482e-02, -2.6542e-02, -5.7482e-02,  2.1037e-03,  3.2330e-02,\n",
      "          1.6018e-02, -6.8087e-02,  6.2649e-02,  3.5315e-02,  1.1822e-02,\n",
      "          3.7212e-03,  1.0973e-03, -3.2089e-02,  2.5410e-05,  1.8270e-02,\n",
      "         -3.0727e-02, -5.8657e-02, -6.4603e-02, -1.7537e-02, -7.7394e-03,\n",
      "          2.8646e-02,  3.6014e-04, -3.7037e-02, -5.2062e-02, -6.6579e-03,\n",
      "         -2.8872e-02, -5.4437e-02,  3.0052e-02, -3.4586e-02, -1.7010e-02,\n",
      "         -6.1950e-02, -8.5817e-03, -5.9731e-02, -8.7727e-02, -1.6919e-02,\n",
      "         -5.5647e-04,  2.4644e-02,  3.9383e-02, -4.0700e-02, -9.8387e-02,\n",
      "         -8.7318e-03, -3.6101e-03, -2.0859e-02, -3.3360e-02, -2.0806e-02,\n",
      "         -5.7804e-02,  1.0422e-02, -7.5704e-03,  4.9071e-02,  4.6601e-03,\n",
      "         -2.1753e-02, -3.5478e-02, -7.0700e-02,  4.8086e-02, -6.1905e-03,\n",
      "         -1.8874e-02,  3.9722e-02,  1.7354e-02,  2.1380e-02,  8.2149e-03,\n",
      "         -7.0203e-03,  2.6811e-02,  2.7719e-02, -2.7086e-02, -4.6475e-02,\n",
      "          5.2837e-03, -4.3353e-02, -2.8427e-02, -6.3651e-02,  6.7016e-03,\n",
      "          2.7362e-02,  1.9784e-02, -1.4149e-02,  1.9462e-02, -3.3761e-03,\n",
      "         -7.8721e-03,  2.9993e-02, -5.2176e-02, -1.0621e-02,  1.8695e-02,\n",
      "         -3.4593e-02,  5.8586e-03,  6.0422e-03, -3.0553e-02, -5.2275e-03,\n",
      "         -1.4228e-02, -3.3665e-02, -2.2340e-02, -8.7536e-02,  1.7754e-02,\n",
      "         -4.9244e-02, -5.8273e-02,  4.5041e-02, -4.8328e-02, -4.5554e-02,\n",
      "         -2.4852e-03, -4.2156e-02, -3.6773e-02, -2.1667e-03, -7.1676e-02,\n",
      "         -5.6762e-02, -1.8036e-02, -3.9770e-02, -5.6033e-02, -8.0266e-02,\n",
      "         -9.7157e-03, -1.7021e-02, -1.4318e-02,  2.8480e-02, -2.2190e-02,\n",
      "         -1.7887e-03,  5.4169e-02, -2.6767e-02, -1.4898e-02,  2.8154e-02,\n",
      "          2.5034e-02, -3.6213e-02, -4.4813e-02, -9.1689e-03, -3.2198e-02,\n",
      "         -1.6636e-02, -1.4978e-02, -2.8206e-02, -4.1953e-02, -4.0311e-02,\n",
      "          5.9899e-02,  4.2443e-02, -8.3173e-02, -2.1016e-02, -3.3347e-02,\n",
      "         -4.4438e-02, -2.0547e-02,  2.1145e-02,  4.3809e-02,  4.7394e-02,\n",
      "          2.6411e-02,  5.4722e-03, -3.6503e-03, -7.2999e-03,  4.5254e-02,\n",
      "         -7.6047e-02,  1.9477e-02, -7.3366e-02, -1.6679e-02, -2.5634e-02,\n",
      "         -3.0964e-02,  5.5001e-03, -5.4133e-02,  5.1056e-02,  6.3931e-02,\n",
      "         -2.4935e-02, -3.3326e-02, -1.1319e-02,  1.9233e-02, -9.6289e-03,\n",
      "         -4.9795e-02,  1.3875e-02, -1.9540e-02, -8.8291e-03, -8.6859e-02,\n",
      "         -2.0432e-02, -5.2432e-02, -2.4241e-02,  3.1216e-02, -3.9785e-02,\n",
      "         -4.6648e-02, -3.9422e-02,  5.7173e-02, -2.9177e-02, -9.5334e-02,\n",
      "          2.1703e-02, -9.0428e-03, -6.5013e-02, -1.1061e-01, -3.4407e-03,\n",
      "         -5.2887e-02, -1.9445e-02,  4.8831e-03, -5.8450e-02, -9.2286e-02,\n",
      "         -6.5292e-02, -1.2930e-02, -8.4700e-02, -1.5953e-02, -7.3099e-03,\n",
      "         -1.5699e-03,  3.4423e-02,  5.4114e-02, -4.3994e-02,  4.6609e-03,\n",
      "          5.0194e-02, -6.8088e-02,  1.8760e-02, -2.4052e-02,  3.4346e-02,\n",
      "         -5.1139e-02, -2.1997e-02, -8.5159e-02, -4.9588e-02, -5.1107e-02,\n",
      "         -3.1411e-02,  6.9119e-03,  4.8961e-02, -6.7452e-02, -9.1206e-02,\n",
      "          6.4104e-03, -6.1130e-02,  3.1270e-02,  4.7101e-03,  7.2501e-02,\n",
      "         -7.9401e-03,  2.2673e-03, -1.0041e-01, -3.0945e-02, -5.7122e-03,\n",
      "         -8.3479e-02,  1.6212e-02, -8.6807e-02, -6.7495e-02, -9.1090e-02,\n",
      "         -7.5114e-02, -5.4963e-02,  1.3542e-02,  3.2677e-02,  1.7948e-03,\n",
      "          2.1669e-02, -7.4197e-02, -4.2839e-03, -4.2448e-03, -3.8043e-03,\n",
      "         -4.3662e-02,  1.9261e-02, -5.5844e-02,  2.1757e-02,  2.2363e-03,\n",
      "          1.5457e-02,  2.2129e-02, -8.7475e-02, -5.0357e-02,  1.6714e-02,\n",
      "         -1.5754e-02, -7.0555e-02,  8.9811e-03, -7.5253e-02,  3.4680e-02,\n",
      "          2.9781e-02, -1.2340e-02, -3.8187e-02, -9.8114e-03, -1.3336e-02,\n",
      "         -5.0382e-02,  3.3591e-03, -2.4408e-02,  2.1342e-02, -3.4077e-02,\n",
      "         -2.3193e-03, -5.4703e-02,  1.2290e-02, -1.9406e-02,  4.2661e-02,\n",
      "         -4.5764e-02,  3.2867e-03,  4.6457e-03,  2.5678e-02,  4.3554e-03,\n",
      "          4.4673e-02,  3.6042e-02, -5.0848e-02, -5.3908e-02, -9.1864e-03,\n",
      "         -1.9508e-02,  2.5916e-02, -1.1468e-02, -8.7909e-03, -1.0544e-02,\n",
      "         -3.5443e-02,  5.5650e-03,  2.5160e-02, -9.0767e-03,  4.5627e-02,\n",
      "         -1.8734e-02, -3.0797e-02, -1.8707e-02, -1.0187e-01, -8.1011e-02,\n",
      "         -1.5662e-02, -8.8661e-02, -8.4613e-02, -3.6600e-03,  1.1971e-02,\n",
      "          4.6272e-02, -2.1290e-02,  2.3129e-02, -7.9258e-02, -1.1582e-01,\n",
      "         -2.4505e-02,  4.0532e-03, -2.7489e-02,  8.4129e-03, -1.7656e-02,\n",
      "         -1.4416e-02, -1.1170e-02, -5.9296e-02, -6.9202e-03,  2.9395e-02,\n",
      "         -6.7834e-02, -8.3561e-02,  4.1367e-02, -5.6588e-02,  1.6671e-02,\n",
      "          7.5029e-03,  2.2772e-02,  6.8327e-02, -1.1417e-02, -5.9277e-02,\n",
      "         -7.8861e-02, -2.4853e-02, -8.2301e-02, -4.6063e-02,  3.7698e-02,\n",
      "          1.0681e-02, -6.9332e-03, -5.0324e-02, -4.1617e-02, -4.3231e-02,\n",
      "          5.3351e-02, -6.6745e-02, -1.1989e-02, -3.8668e-02,  4.3444e-02,\n",
      "         -6.7733e-02,  3.1424e-02, -1.8017e-03,  1.4843e-02, -3.2234e-03,\n",
      "         -3.4876e-02,  1.3246e-02, -2.9505e-02,  2.8412e-02,  1.7895e-02,\n",
      "          5.7844e-03, -4.2882e-02,  2.6622e-02,  1.4931e-02, -2.1757e-02,\n",
      "         -2.1075e-02,  4.6209e-02, -3.7092e-02, -2.7483e-02, -1.2659e-02,\n",
      "         -2.9806e-02, -1.1591e-02,  1.2242e-02, -3.9037e-02, -1.1860e-02,\n",
      "          1.8619e-02, -4.5176e-02, -7.7971e-02, -3.9850e-02, -6.5438e-02,\n",
      "          3.5440e-02,  2.5308e-02, -5.8435e-02, -1.8811e-03, -2.8518e-02,\n",
      "         -6.0963e-02, -1.1092e-01,  2.0978e-02,  9.8720e-03, -3.2421e-02,\n",
      "         -1.0786e-01, -1.1143e-04, -6.9075e-02, -7.8690e-02,  3.3350e-02,\n",
      "          5.0108e-03, -3.8232e-02, -5.0940e-02, -4.7356e-02, -2.3847e-02,\n",
      "         -8.8199e-02, -2.1803e-02, -2.5640e-02,  1.1859e-02, -9.4681e-02,\n",
      "          5.9033e-02, -2.7911e-02, -7.4125e-02,  1.3654e-02, -7.4683e-02,\n",
      "         -2.6221e-02,  3.9234e-02,  3.8811e-02, -3.6114e-02, -6.5355e-02,\n",
      "         -5.3331e-02,  2.3426e-02, -1.0981e-02, -2.8386e-02, -3.8378e-02,\n",
      "         -3.8951e-02, -3.7126e-02, -9.1236e-02, -4.0407e-02, -4.0821e-03,\n",
      "         -1.1980e-02, -5.9070e-02, -5.4969e-02, -3.1875e-02, -4.9518e-02,\n",
      "          3.7196e-04, -2.4913e-02, -1.8567e-02, -4.7982e-02, -2.1875e-02,\n",
      "         -1.0688e-02, -4.7933e-02, -1.7806e-02,  1.6425e-02, -2.8015e-02,\n",
      "         -5.1616e-02, -9.4458e-03, -8.1441e-02, -7.3196e-02,  2.5089e-02,\n",
      "          3.1195e-03, -4.0170e-03, -7.0565e-02, -1.0268e-01,  5.6357e-02,\n",
      "          7.2759e-02,  1.5875e-02, -3.2757e-03,  2.4413e-02,  5.2270e-02,\n",
      "          3.6066e-02,  1.1467e-02,  9.7023e-04,  2.9172e-03, -3.0505e-02,\n",
      "         -6.4037e-03,  1.7488e-02, -2.1797e-02,  1.7163e-02,  8.0928e-03,\n",
      "         -3.5020e-02, -7.7396e-02, -3.3842e-02, -3.1056e-02,  7.5218e-03,\n",
      "         -9.6323e-03,  2.3698e-02, -1.4847e-02, -5.5555e-02, -5.3886e-03,\n",
      "         -1.8770e-02,  7.9117e-02, -1.6056e-02, -1.7292e-02, -3.8195e-02,\n",
      "          2.6017e-02, -4.5755e-02, -6.5598e-03,  1.6035e-03,  7.3584e-02,\n",
      "         -7.5565e-02, -7.5782e-02, -6.0178e-02,  1.6869e-02,  2.5808e-02,\n",
      "          7.7043e-03, -3.9930e-02, -3.8114e-03,  7.1530e-02, -1.4234e-02,\n",
      "          4.2238e-05, -4.9247e-02,  7.1346e-02,  2.6794e-02,  4.1325e-02,\n",
      "          1.8150e-02, -7.8744e-02, -8.0625e-03,  4.2598e-02, -9.1964e-02,\n",
      "         -8.0097e-02, -9.4992e-03, -8.0187e-02, -1.3076e-02,  4.7093e-03,\n",
      "         -6.3652e-02,  7.1776e-02,  1.8090e-03, -6.0156e-02,  4.7806e-03,\n",
      "          3.8947e-03, -5.8476e-02, -1.3171e-02,  2.0067e-02, -5.5717e-02,\n",
      "          1.8472e-02,  5.4452e-02, -4.4256e-02,  4.0274e-02,  1.7242e-02,\n",
      "         -9.0756e-02,  4.2329e-03, -3.6412e-02,  3.2679e-02,  5.5789e-02,\n",
      "          1.9003e-02, -6.1356e-02, -3.6005e-02,  3.5700e-03,  9.0347e-04,\n",
      "         -7.3200e-02, -4.8060e-02, -6.1942e-02, -3.9321e-02, -2.0603e-02,\n",
      "          2.8123e-02, -1.3196e-02,  2.3674e-02, -9.5218e-02, -4.7920e-02,\n",
      "         -1.8811e-02, -3.8724e-02, -1.8437e-03,  2.2110e-02, -2.9245e-02,\n",
      "          3.6088e-02,  1.5044e-03, -4.8473e-02,  3.4779e-02,  2.4720e-02,\n",
      "         -9.7820e-03,  4.3833e-02, -4.0755e-02,  3.4088e-02, -5.4636e-02,\n",
      "         -4.3619e-02,  1.3160e-02, -4.7641e-03, -5.9633e-03, -2.0940e-02,\n",
      "         -2.2532e-03, -2.0453e-02, -1.1275e-02, -8.1422e-02, -4.5355e-02,\n",
      "          1.3038e-02, -2.6077e-02, -2.9745e-02, -5.2280e-02, -2.7975e-03,\n",
      "         -1.7751e-02, -2.9460e-03, -7.4038e-02]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# get the embedding vector for the word \"example\"\n",
    "example_token_id = tokenizer.convert_tokens_to_ids([\"example\"])[0]\n",
    "example_embedding = model.embeddings.word_embeddings(torch.tensor([example_token_id]))\n",
    "\n",
    "print(example_embedding.shape)\n",
    "print(example_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe weight matrix of the embedding layer contains small, random values. \\nThese values are optimized during LLM training as part of the LLM optimization itself. \\nMoreover, we can see that the weight matrix has six rows and three columns. \\nThere is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE tokenizer vocab has 50,257 words, and GPT3 has embedding size of 12,288\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# we can instantiate an embedding layer in Pytorch \n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "'''\n",
    "The weight matrix of the embedding layer contains small, random values. \n",
    "These values are optimized during LLM training as part of the LLM optimization itself. \n",
    "Moreover, we can see that the weight matrix has six rows and three columns. \n",
    "There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf we compare the embedding vector for token ID 3 to the previous embedding matrix, \\nwe see that it is identical to the fourth row (Python starts with a zero index, so it’s the row corresponding to index 3). \\nIn other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply it to tokenID to obtain the embedding vector\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "\n",
    "'''\n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, \n",
    "we see that it is identical to the fourth row (Python starts with a zero index, so it’s the row corresponding to index 3). \n",
    "In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In self-attention, our goal is to calculate context vectors z(i) for each element x(i) in the input sequence. \n",
    "# A context vector can be interpreted as an enriched embedding vector.\n",
    "# Context vectors purpose is to create enriched representations of each element in an input sequence by incorporating information from all other elements in the sequence.\n",
    "# This is essential in LLMs, which need to understand the relationship and relevance of words in a sentence to each other.\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "# ex: z(2) is an embedding that contains information about x(2) and all other input elements x(1) to x(T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.4300, 0.1500, 0.8900])\n",
      "1 tensor([0.5500, 0.8700, 0.6600])\n",
      "2 tensor([0.5700, 0.8500, 0.6400])\n",
      "3 tensor([0.2200, 0.5800, 0.3300])\n",
      "4 tensor([0.7700, 0.2500, 0.1000])\n",
      "5 tensor([0.0500, 0.8000, 0.5500])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBeyond viewing the dot product operation as a mathematical tool that combines two vectors to yield a scalar value, \\nthe dot product is a measure of similarity because it quantifies how closely two vectors are aligned: \\na higher dot product indicates a greater degree of alignment or similarity between the vectors. \\nIn the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or “attends to,” any other element: \\nthe higher the dot product, the higher the similarity and attention score between two elements.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first step of self-attention is to compute the intermediate values, w, referred as attention scores. \n",
    "# We determine these scores by computing the dot product of the query, x(2), with every other input token\n",
    "\n",
    "# Example: Compute attention score for \"journey x(2)\"\n",
    "query = inputs[1]\n",
    "attn_score2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i,x_i)\n",
    "    attn_score2[i] = torch.dot(x_i, query)\n",
    "print(attn_score2)\n",
    "\n",
    "'''\n",
    "Beyond viewing the dot product operation as a mathematical tool that combines two vectors to yield a scalar value, \n",
    "the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: \n",
    "a higher dot product indicates a greater degree of alignment or similarity between the vectors. \n",
    "In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or “attends to,” any other element: \n",
    "the higher the dot product, the higher the similarity and attention score between two elements.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In the next step, we normalize each of the attention scores we computed previously. \n",
    "The main goal behind the normalization is to obtain attention weights that sum up to 1. \n",
    "This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. \n",
    "'''\n",
    "\n",
    "attn_weights_2_tmp = attn_score2 / attn_score2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n",
    "\n",
    "'''\n",
    "In practice, it’s more common and advisable to use the softmax function for normalization. \n",
    "This approach is better at managing extreme values and offers more favorable gradient properties during training. \n",
    "The following is a basic implementation of the softmax function for normalizing the attention scores using Pytorch:\n",
    "'''\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_score2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now that we have computed the normalized attention weights, we are ready for the final step: \n",
    "calculating the context vector z(2) by multiplying the embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors.\n",
    "Thus, context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:\n",
    "'''\n",
    "\n",
    "query = inputs[1]         \n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Computing attention weights for all input tokens\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n",
    "print()\n",
    "'''\n",
    "When computing the preceding attention score tensor, we used for loops in Python. \n",
    "However, for loops are generally slow, and we can achieve the same results using matrix multiplication:\n",
    "'''\n",
    "\n",
    "# Attention matrix: the matrix of attention scores computed in a Transformer's self-attention mechanism. \n",
    "# It determines how much focus each word gives to each word in a sequence.\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)\n",
    "print()\n",
    "\n",
    "# Compute attention weights: normalization\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)\n",
    "print()\n",
    "\n",
    "# Compute context vectors\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the attention weights \n",
    "'''\n",
    "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices Wq, Wk, and Wv. \n",
    "These three matrices are used to project the embedded input tokens, x(i), into query, key, and value vectors, respectively.\n",
    "\n",
    "Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, \n",
    "we’ll use different input (d_in=3) and output (d_out=2) dimensions here.\n",
    "'''\n",
    "x_2 = inputs[1]     #1\n",
    "d_in = inputs.shape[1]      #2\n",
    "d_out = 2         #3\n",
    "\n",
    "#1 The second input element\n",
    "#2 The input embedding size, d=3\n",
    "#3 The output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) tensor([0.4433, 1.1419]) tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# Then we initialize the thre weight matrices Wq, Wk, and Wv. 3x2 dim\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Then we compute the query, key, and value vectors\n",
    "query_2 = x_2 @ W_query # 1x3 x 3x2 => 1x2 \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2, key_2, value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Even though our temporary goal is only to compute the one context vector, z(2), \n",
    "we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query q (2)\n",
    "'''\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Step2: Compute Attention Scores\n",
    "attn_scores_2 = query_2 @ keys.T       #1\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now, we want to go from the attention scores to the attention weights, as illustrated in figure 3.16. \n",
    "We compute the attention weights by scaling the attention scores and using the softmax function. \n",
    "However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys \n",
    "(taking the square root is mathematically the same as exponentiating by 0.5):\n",
    "'''\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Final step: compute context vectors\n",
    "\n",
    "'''\n",
    "Similar to when we computed the context vector as a weighted sum over the input vectors (see section 3.3), \n",
    "we now compute the context vector as a weighted sum over the value vectors. \n",
    "Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. \n",
    "Also as before, we can use matrix multiplication to obtain the output in one step:\n",
    "'''\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)\n",
    "\n",
    "'''\n",
    "So far, we’ve only computed a single context vector, z(2). \n",
    "Next, we will generalize the code to compute all context vectors in the input sequence, z(1) to z(T).\n",
    "'''\n",
    "\n",
    "# input: 6x3\n",
    "# Wq, Wk, Wv: 3x2 each \n",
    "# q(t) = x(t) x Wq: 1x3 x 3x2 = 1x2\n",
    "# key = input x Wk = 6x3 x 3x2 = 6x2 (same for value)\n",
    "# attention score for x(t): query_t @ key.T = 1x2 x 2x6 = 1x6 \n",
    "# attention weights for x(t): softmax(attention score of x(t) / embedded dim of key = 2) = 1x6\n",
    "# context vector z(t): attention weights of x(t) @ values = 1x6 @ 6x2 => 1x2\n",
    "\n",
    "# Query: the current item the model focuses on or tries to understand\n",
    "# Key: each item in the input sequence has an associated key, these keys are used to match the query \n",
    "# Value: It represents the actual content or representation of the input items. Once the model determines which keys are most relevant to the query, it retrieves the corresponding values. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
